seed: 42

model_path: "pretrained_models/stable-diffusion-v1-5"
vae_path: "pretrained_models/sd-vae-ft-mse"
# vae_path: "ft_vae/checkpoint-11000"
clip_model_path: 'pretrained_models/clip-vit-base-patch32'

unet_path: "outputs_stage1_freeze_refer_Talk/checkpoint-40000"
pretrained_poseguider_path: "outputs_stage1_freeze_refer_Talk/checkpoint-40000/pose.ckpt"
pretrained_referencenet_path: 'outputs_stage1_freeze_refer_TikTok/checkpoint-80000'

# unet_path: "outputs_stage1_freeze_refer_TikTok/checkpoint-80000"
# pretrained_poseguider_path: "outputs_stage1_freeze_refer_TikTok/checkpoint-80000/pose.ckpt"
# pretrained_referencenet_path: 'outputs_stage1_freeze_refer_TikTok/checkpoint-80000'

# unet_path: "outputs_stage1_freeze_refer/checkpoint-240000"
# pretrained_poseguider_path: "outputs_stage1_freeze_refer/checkpoint-240000/pose.ckpt"
# pretrained_referencenet_path: 'outputs_stage1_freeze_refer/checkpoint-240000'


out_dir: 'image_output_stage1_test_Talk'

batch_size: 2
dataloader_num_workers: 4
guidance_scale: 7.5

infer_data:
  csv_path:     "./data/Talk_test_info.csv"
  video_folder: "../Talk_dataset"
  sample_size:  512 # for 40G 256
  sample_stride: 4
  sample_n_frames: 8
  clip_model_path: 'pretrained_models/clip-vit-base-patch32'
  sub_folder: 'deal_test'

# infer_data:
#   # csv_path:     "./data/UBC_train_info_test.csv"
#   csv_path:     "./data/TikTok_info.csv"
#   video_folder: "../TikTok_dataset2/TikTok_dataset"
#   sample_size:  512 # for 40G 256
#   sample_stride: 4
#   sample_n_frames: 8
#   clip_model_path: 'pretrained_models/clip-vit-base-patch32'

# infer_data:
#   # csv_path:     "./data/UBC_train_info_test.csv"
#   csv_path:     "./data/UBC_test_info.csv"
#   video_folder: "../UBC_dataset"
#   is_train: false
#   sample_size:  512 # for 40G 256
#   sample_stride: 4
#   sample_n_frames: 8
#   clip_model_path: 'pretrained_models/clip-vit-base-patch32'

fusion_blocks: "full"
image_finetune: true